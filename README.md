<!-- BEGIN_DIALECT_LINK -->

## Dialectal Audio Examples

**Listen in your browser:** ğŸ‘‰ [Dialect Examples (with audio players)](https://labspire.github.io/respin_did_interspeech25/examples.html)

<!-- END_DIALECT_LINK -->

# Jointly Improving Dialect Identification and ASR in Indian Languages using Multimodal Feature Fusion

Official implementation of the paper:

> **Saurabh Kumar, Amartyaveer, Prasanta Kumar Ghosh**  
> *[Jointly Improving Dialect Identification and ASR in Indian Languages using Multimodal Feature Fusion](https://www.isca-archive.org/interspeech_2025/kumar25_interspeech.html#)*  
> Interspeech 2025

---

## ğŸ“Œ Overview

Dialectal variation poses a major challenge to **Automatic Speech Recognition (ASR)** in Indian languages.  
This work addresses it by **jointly training ASR and Dialect Identification (DID)** models using **multimodal feature fusion**.

We propose **ASR-BN-ROB**, a joint architecture that combines:
- **Bottleneck Encoder** â€“ captures acoustic and temporal cues.  
- **RoBERTa Encoder** â€“ extracts lexical/semantic cues from ASR CTC embeddings.  
- **Gating Mechanism** â€“ adaptively fuses speech and text features.  
- **Feedback to ASR Encoder** â€“ improves ASR without prepending dialect tokens.  

---

## ğŸš€ Key Contributions

- **Improved DID**: 81.63% accuracy across 33 dialects  
- **Better ASR**: CER = 4.65%, WER = 17.73%  
- **Breaks the ASRâ€“DID trade-off**: improves both tasks simultaneously  
- **Robustness**: maintains ASR performance even with incorrect DID predictions  
- **Reproducibility**: ESPnet-based code + pretrained models  

---

## ğŸ“Š Dataset

We evaluate on the **[RESPIN Corpus](https://spiredatasets.ee.iisc.ac.in/respincorpus)** (SPIRE Lab, IISc).  

- **Languages**: Bhojpuri, Bengali, Chhattisgarhi, Kannada, Magahi, Marathi, Maithili, Telugu  
- **Dialects**: 33 total  
- **Training data**: ~140â€“175 hrs read speech per language  
- **Setup**: Small train set is used for all languages in this work  

---

## ğŸ— Model Architecture

- **Speech features** â†’ SSL Conformer Encoder â†’ Bottleneck Encoder  
- **Text features** â†’ RoBERTa Encoder on CTC embeddings  
- **Fusion** â†’ Gating mechanism + Attention Encoder  
- **Loss** â†’ Hybrid CTC+Attention ASR loss + DID cross-entropy loss  

---

## ğŸ“ˆ Results

**Dialect Identification (Accuracy %):**

| System                | Avg. Accuracy |
|-----------------------|---------------|
| ASR-DID-ROB (baseline)| 80.74         |
| **ASR-BN-ROB (Proposed)** | **81.63** |

**ASR Performance:**

| System      | CER (%) | WER (%) |
|-------------|---------|---------|
| Base-ASR    | 4.81    | 18.38   |
| ASR-DID-ROB | 4.76    | 18.16   |
| **ASR-BN-ROB** | **4.65** | **17.73** |

---

## ğŸ“‚ Repository Structure

```
respin_did_interspeech25/
â”‚
â”œâ”€â”€ espnet_mods/ # Modified + new ESPnet files (commit 6b5f5269a2a6d8902fe3697f88ca9c0ccde35353)
â”‚ â”œâ”€â”€ nets/...
â”‚ â”œâ”€â”€ bin/...
â”‚ â”œâ”€â”€ task/...
â”‚ â””â”€â”€ train/...
â”‚
â”œâ”€â”€ samples/ # Example audio samples
â”œâ”€â”€ text_files/ # Example transcripts
â”œâ”€â”€ run_all_asr_did.sh # Multi-language training + evaluation script
â”œâ”€â”€ requirements.txt # Python dependencies (pip freeze)
â”œâ”€â”€ examples.html # Demo page with audio players
â””â”€â”€ README.md # This file
```


---

## âš™ï¸ Installation

1. **Clone ESPnet (specific commit):**
    ```bash
    git clone https://github.com/espnet/espnet.git
    cd espnet
    git checkout 6b5f5269a2a6d8902fe3697f88ca9c0ccde35353
    ```

2. **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

3. **Copy modified files:**
    ```bash
    cp -r path_to_repo/espnet_mods/* espnet/
    ```

### â–¶ï¸ Training & Evaluation

Example command for training:

```bash
cd espnet/egs2/respin_did_is25/asr1
./run_all_asr_did.sh --stage 1 --stop_stage 13
```

---

## ğŸ“œ Citation

If you use this code or models in your work, please cite:

```bibtex
@inproceedings{kumar25_interspeech,
  title     = {{Jointly Improving Dialect Identification and ASR in Indian Languages using Multimodal Feature Fusion}},
  author    = {{Saurabh Kumar and  Amartyaveer and Prasanta Kumar Ghosh}},
  year      = {{2025}},
  booktitle = {{Interspeech 2025}},
  pages     = {{2770--2774}},
  doi       = {{10.21437/Interspeech.2025-421}},
  issn      = {{2958-1796}},
}
```

---

## ğŸ“¬ Contact
- Saurabh Kumar â€“ saurabhk0317@gmail.com
- SPIRE Lab, IISc Bangalore â€“ spirelab.ee@iisc.ac.in
- Project Page â€“ [RESPIN](https://respin.iisc.ac.in/)
- Code â€“ [GitHub Repository](https://github.com/labspire/respin_did_interspeech25.git)